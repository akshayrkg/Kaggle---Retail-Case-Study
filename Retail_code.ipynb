{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import xgboost as xgb\n",
    "import pylab as pl\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wther2007=pd.read_excel('WeatherData.xlsx','2009')\n",
    "#Wther2008=pd.read_excel('WeatherData.xlsx','2010')\n",
    "Wther2009=pd.read_excel('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/WeatherData.xlsx','2011')\n",
    "Wther2010=pd.read_excel('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/WeatherData.xlsx','2012')\n",
    "Wther2011=pd.read_excel('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/WeatherData.xlsx','2013')\n",
    "Wther2012=pd.read_excel('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/WeatherData.xlsx','2014')\n",
    "Wther2013=pd.read_excel('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/WeatherData.xlsx','2015')\n",
    "Wther2014=pd.read_excel('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/WeatherData.xlsx','2016')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wther=Wther.drop(['Temp high (°C)','Temp low (°C)','Dew Point high (°C)','Dew Point low (°C)','Humidity\\xa0(%) high','Humidity\\xa0(%) low','Sea Level Press.\\xa0(hPa) high','Sea Level Press.\\xa0(hPa) low','Visibility\\xa0(km) high','Visibility\\xa0(km) low','Wind\\xa0(km/h) high','Wind\\xa0(km/h) low','WeatherEvent','Precip.\\xa0(mm) sum'],axis=1)\n",
    "\n",
    "#Wther2007=Wther2007.drop(['Temp high (°C)','Temp low (°C)','Dew Point high (°C)','Dew Point low (°C)','Humidity\\xa0(%) high','Humidity\\xa0(%) low','Sea Level Press.\\xa0(hPa) high','Sea Level Press.\\xa0(hPa) low','Visibility\\xa0(km) high','Visibility\\xa0(km) low','Wind\\xa0(km/h) high','Wind\\xa0(km/h) low','WeatherEvent','Precip.\\xa0(mm) sum'],axis=1)\n",
    "#Wther2008=Wther2008.drop(['Temp high (°C)','Temp low (°C)','Dew Point high (°C)','Dew Point low (°C)','Humidity\\xa0(%) high','Humidity\\xa0(%) low','Sea Level Press.\\xa0(hPa) high','Sea Level Press.\\xa0(hPa) low','Visibility\\xa0(km) high','Visibility\\xa0(km) low','Wind\\xa0(km/h) high','Wind\\xa0(km/h) low','WeatherEvent','Precip.\\xa0(mm) sum'],axis=1)\n",
    "Wther2009=Wther2009.drop(['Temp high (°C)','Temp low (°C)','Dew Point high (°C)','Dew Point low (°C)','Humidity\\xa0(%) high','Humidity\\xa0(%) low','Sea Level Press.\\xa0(hPa) high','Sea Level Press.\\xa0(hPa) low','Visibility\\xa0(km) high','Visibility\\xa0(km) low','Wind\\xa0(km/h) high','Wind\\xa0(km/h) low','WeatherEvent','Precip.\\xa0(mm) sum'],axis=1)\n",
    "Wther2010=Wther2010.drop(['Temp high (°C)','Temp low (°C)','Dew Point high (°C)','Dew Point low (°C)','Humidity\\xa0(%) high','Humidity\\xa0(%) low','Sea Level Press.\\xa0(hPa) high','Sea Level Press.\\xa0(hPa) low','Visibility\\xa0(km) high','Visibility\\xa0(km) low','Wind\\xa0(km/h) high','Wind\\xa0(km/h) low','WeatherEvent','Precip.\\xa0(mm) sum'],axis=1)\n",
    "Wther2011=Wther2011.drop(['Temp high (°C)','Temp low (°C)','Dew Point high (°C)','Dew Point low (°C)','Humidity\\xa0(%) high','Humidity\\xa0(%) low','Sea Level Press.\\xa0(hPa) high','Sea Level Press.\\xa0(hPa) low','Visibility\\xa0(km) high','Visibility\\xa0(km) low','Wind\\xa0(km/h) high','Wind\\xa0(km/h) low','WeatherEvent','Precip.\\xa0(mm) sum'],axis=1)\n",
    "Wther2012=Wther2012.drop(['Temp high (°C)','Temp low (°C)','Dew Point high (°C)','Dew Point low (°C)','Humidity\\xa0(%) high','Humidity\\xa0(%) low','Sea Level Press.\\xa0(hPa) high','Sea Level Press.\\xa0(hPa) low','Visibility\\xa0(km) high','Visibility\\xa0(km) low','Wind\\xa0(km/h) high','Wind\\xa0(km/h) low','WeatherEvent','Precip.\\xa0(mm) sum'],axis=1)\n",
    "Wther2013=Wther2013.drop(['Temp high (°C)','Temp low (°C)','Dew Point high (°C)','Dew Point low (°C)','Humidity\\xa0(%) high','Humidity\\xa0(%) low','Sea Level Press.\\xa0(hPa) high','Sea Level Press.\\xa0(hPa) low','Visibility\\xa0(km) high','Visibility\\xa0(km) low','Wind\\xa0(km/h) high','Wind\\xa0(km/h) low','WeatherEvent','Precip.\\xa0(mm) sum'],axis=1)\n",
    "Wther2014=Wther2014.drop(['Temp high (°C)','Temp low (°C)','Dew Point high (°C)','Dew Point low (°C)','Humidity\\xa0(%) high','Humidity\\xa0(%) low','Sea Level Press.\\xa0(hPa) high','Sea Level Press.\\xa0(hPa) low','Visibility\\xa0(km) high','Visibility\\xa0(km) low','Wind\\xa0(km/h) high','Wind\\xa0(km/h) low','WeatherEvent','Precip.\\xa0(mm) sum'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wther=pd.concat([Wther2009,Wther2010,Wther2011,Wther2012,Wther2013,Wther2014],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year={2009:1,2010:2,2011:3,2012:4,2013:5,2014:6}\n",
    "month={'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, 'Aug':8, 'Sep':9,\n",
    "       'Oct':10, 'Nov':11, 'Dec':12}\n",
    "Wther[\"Year\"] = Wther[\"Year\"].map(year)\n",
    "Wther[\"Month\"] = Wther[\"Month\"].map(month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Dealing with Null values and changing categories to calculate average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wther[Wther.values  == \"-\"]\n",
    "#Wther=Wther.replace('-',np.NaN)\n",
    "#Wther[pd.isnull(Wther).any(axis=1)]\n",
    "#pd.to_numeric(Wthertry['Temp avg (°C)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Wther.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wther_avg=Wther.groupby(['Year','Month'])['Temp avg (°C)','Dew Point avg (°C)','Humidity\\xa0(%) avg','Sea Level Press.\\xa0(hPa) avg','Visibility\\xa0(km) avg','Wind\\xa0(km/h) avg'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wther[(Wther.Year==1) & (Wther.Month==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wther_avg=Wther_avg.values[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wther_fin=pd.DataFrame(Wther_avg,columns=['Temp_avg','Dew_Point_avg','Humidity_avg','Sea_Level_Press_avg','Visibility_avg','Wind_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Wther_fin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Train_Kaggle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Women=data[data['ProductCategory']=='WomenClothing']\n",
    "#print(data_Women.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Women=data_Women.fillna(value=3293)\n",
    "#data_Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Other=data[data['ProductCategory']=='OtherClothing']\n",
    "#data_Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Other=data_Other.fillna(value=1107)\n",
    "#data_Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Men=data[data['ProductCategory']=='MenClothing']\n",
    "#data_Men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Men=data_Men.fillna(value=674)\n",
    "#data_Men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_data=pd.concat([data_Men,data_Women,data_Other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin_data\n",
    "dept={'MenClothing':1,'OtherClothing':2,'WomenClothing':3}\n",
    "fin_data[\"ProductCategory\"] = fin_data[\"ProductCategory\"].map(dept)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year={2009:1,2010:2,2011:3,2012:4,2013:5,2014:6}\n",
    "fin_data[\"Year\"] = fin_data[\"Year\"].map(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fin=fin_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fin_Men=train_fin[train_fin['ProductCategory']==1].reset_index(drop=True)\n",
    "train_fin_Others=train_fin[train_fin['ProductCategory']==2].reset_index(drop=True)\n",
    "train_fin_Women=train_fin[train_fin['ProductCategory']==3].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_fin_Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holiday Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holi=pd.read_excel('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/Events_HolidaysData.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holi_fin=holi.drop(['Year','Month'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "holi_fin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtr=pd.read_csv('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/macrotrain')\n",
    "mte=pd.read_csv('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/macrotest').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_men.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_men=pd.concat([train_fin_Men,holi_fin,Wther_fin,mtr],axis=1)\n",
    "full_others=pd.concat([train_fin_Others,holi_fin,Wther_fin,mtr],axis=1)\n",
    "full_women=pd.concat([train_fin_Women,holi_fin,Wther_fin,mtr],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = full_men['Sales(In ThousandDollars)']\n",
    "full_men.drop(labels=['Sales(In ThousandDollars)'], axis=1,inplace = True)\n",
    "full_men.insert(22, 'Sales(In ThousandDollars)', cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = full_others['Sales(In ThousandDollars)']\n",
    "full_others.drop(labels=['Sales(In ThousandDollars)'], axis=1,inplace = True)\n",
    "full_others.insert(22, 'Sales(In ThousandDollars)', cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = full_women['Sales(In ThousandDollars)']\n",
    "full_women.drop(labels=['Sales(In ThousandDollars)'], axis=1,inplace = True)\n",
    "full_women.insert(22, 'Sales(In ThousandDollars)', cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_others.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_men_X=full_men.values[:,:10]\n",
    "full_men_Y=full_men.values[:,10:]\n",
    "full_others_X=full_others.values[:,:10]\n",
    "full_others_Y=full_others.values[:,10:]\n",
    "full_women_X=full_women.values[:,:10]\n",
    "full_women_Y=full_women.values[:,10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_men_X.shape,full_others_X.shape,full_women_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Testing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('Test_Kaggle.csv')\n",
    "\n",
    "test_data=test_data.drop('Sales(In ThousandDollars)',axis=1)\n",
    "\n",
    "dept={'MenClothing':1,'OtherClothing':2,'WomenClothing':3}\n",
    "test_data[\"ProductCategory\"] = test_data[\"ProductCategory\"].map(dept)\n",
    "\n",
    "yeart={2015:7}\n",
    "test_data[\"Year\"] = test_data[\"Year\"].map(yeart)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_Men=test_data[test_data['ProductCategory']==1].reset_index(drop=True)\n",
    "test_data_Others=test_data[test_data['ProductCategory']==2].reset_index(drop=True)\n",
    "test_data_Women=test_data[test_data['ProductCategory']==3].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wther_test=Wther_fin[60:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holi_test=holi_fin[60:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_men=pd.concat([test_data_Men,holi_test,Wther_test,mte],axis=1)\n",
    "test_others=pd.concat([test_data_Others,holi_test,Wther_test,mte],axis=1)\n",
    "test_women=pd.concat([test_data_Women,holi_test,Wther_test,mte],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data_test=pd.concat([test_men,test_women,test_others])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "big_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ('M','W','O'):\n",
    "    if i=='M':\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(full_men_X,full_men_Y,test_size=0.15,random_state=42)\n",
    "    if i=='W':\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(full_women_X,full_women_Y,test_size=0.15,random_state=42)\n",
    "    if i=='O':\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(full_others_X,full_others_Y,test_size=0.15,random_state=42)\n",
    "    for i in range(11):\n",
    "        gb=GradientBoostingRegressor(learning_rate=0.4,n_estimators=750,loss='ls',criterion='mae',warm_start=True)\n",
    "        gb.fit(X_train,Y_train.ravel())\n",
    "        Y_pred_gb = gb.predict(X_val)\n",
    "        print(\"Mean squared error: %.2f\"% np.sqrt(mean_squared_error(Y_val, Y_pred_gb)))\n",
    "\n",
    "#gb3=GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data=pd.concat([full_men,full_women,full_others])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#big_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data_X=big_data.values[:,:22]\n",
    "big_data_Y=big_data.values[:,22:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "big_data_X.shape,big_data_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''np.save('X_train',X_train)\n",
    "np.save('Y_train',Y_train)\n",
    "np.save('X_val',X_val)\n",
    "np.save('Y_val',Y_val)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.load('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/X_train.npy')\n",
    "Y_train=np.load('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/Y_train.npy')\n",
    "X_val=np.load('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/X_val.npy')\n",
    "Y_val=np.load('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/Y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(big_data_X,big_data_Y,test_size=0.01,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#xgbr = xgb.XGBRegressor(learning_rate=0.111111111111111, n_estimators=725)\n",
    "n=301\n",
    "if n==301:\n",
    "#for n in range(11,31):    \n",
    "    print(n)\n",
    "    gb=GradientBoostingRegressor(learning_rate=0.33333333333333333,n_estimators=n,loss='ls',criterion='mae')#,subsample=0.75)\n",
    "    #rreg=RandomForestRegressor(n_estimators=n,criterion='mse')\n",
    "    #ab=AdaBoostRegressor(gb)\n",
    "    #gb=GradientBoostingRegressor(learning_rate=0.33333333333333333,n_estimators=)#17 threes\n",
    "    train_plot=[]\n",
    "    val_plot=[]\n",
    "    for i in range(11):\n",
    "        #ab=AdaBoostRegressor(gb,n_estimators=70, random_state=42,learning_rate=0.5)\n",
    "        gb.fit(X_train,Y_train.ravel())\n",
    "        Y_pred_gb = gb.predict(X_val)\n",
    "        Y_pred_gb_train=gb.predict(X_train)\n",
    "        print(\"Mean squared error train: %.2f\"% np.sqrt(mean_squared_error(Y_train, Y_pred_gb_train)))\n",
    "        train_plot.append(np.sqrt(mean_squared_error(Y_train, Y_pred_gb_train)))\n",
    "        print(\"Mean squared error val: %.2f\"% np.sqrt(mean_squared_error(Y_val, Y_pred_gb)))\n",
    "        val_plot.append(np.sqrt(mean_squared_error(Y_val, Y_pred_gb)))\n",
    "        print('----------------------')\n",
    "#plt.plot(list(range(11)),train_plot,'-')\n",
    "#plt.plot(list(range(11)),val_plot,'--')\n",
    "#plt.show()\n",
    "        \n",
    "#gb=GradientBoostingRegressor(learning_rate=0.33333333333333,loss='ls',criterion='mae')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(gb, open(\"xxxx\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=50, test_size=0.1, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters={'n_estimators': range(1,900,100),'learning_rate':pl.frange(0.1,0.9,0.1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search=GridSearchCV(gb,parameters,cv=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train,Y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_,grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred_test = gb.predict(big_data_test.values)\n",
    "#big_data_test[pd.isnull(big_data_test).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=pickle.load(open('/Users/kaku/Downloads/AIML/IIIT-H/Retailathon/model_129.868.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=[]\n",
    "for i in range(12):\n",
    "    sub.append(Y_pred_test[i+12])\n",
    "    sub.append(Y_pred_test[i])\n",
    "    sub.append(Y_pred_test[i+24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsub=np.asarray(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Big Data Test\n",
    "\n",
    "#fil=open('Retail_11.csv','w')\n",
    "#fil=open('Retail_12.csv','w') ###new grid search\n",
    "#fil=open('Retail_13.csv','w')  ## split=0.15, max_depth=None\n",
    "#fil=open('Retail_14.csv','w')  ## split=0.15, max_depth=4\n",
    "#fil=open('Retail_15.csv','w')  ## minor changes\n",
    "#fil=open('Retail_16.csv','w')  ## Adaboost\n",
    "#fil=open('Retail_16.csv','w')  ## gb 0.4 ,900,ls,mae again\n",
    "#fil=open('Retail_17.csv','w')  ## gb 0.111 ,850,ls,mae\n",
    "#fil=open('Retail_17_recreate.csv','w')  ### Recreating Retail_17 for submission\n",
    "#fil=open('Retail_18.csv','w')  ### XGBoost\n",
    "#fil=open('Retail_19.csv','w')  ### XGBoost with RMS=104\n",
    "#fil=open('Retail_20.csv','w')  ### XGBoost with lr=0.15000...2,nestimator=100\n",
    "#fil=open('Retail_21.csv','w')  ### gb 0.1111 ,900,ls,mae\n",
    "#fil=open('Retail_22.csv','w')  ### gb 0.1111 ,2000,ls,mae\n",
    "#fil=open('Retail_23.csv','w')  ### gb 0.15000000000002 ,2000,ls,mae,RMS=105\n",
    "#fil=open('Retail_24.csv','w')  ### gb 0.3333333333333333 ,100,ls,mae,RMS=94\n",
    "#fil=open('Retail_25.csv','w')  ### xgbr 0.111111111111111 ,725\n",
    "#fil=open('Retail_26.csv','w')  ### gb 0.3333333333333333 ,29,ls,mae\n",
    "#fil=open('Retail_27.csv','w')  ### gb 0.3333333333333333 ,40,ls,mae\n",
    "#fil=open('Retail_28.csv','w')  ### gb 0.3333333333333333 ,16,ls,mae,RMS=111\n",
    "#fil=open('Retail_29.csv','w')  ### gb 0.3333333333333333 ,1111,ls,mae\n",
    "#fil=open('Retail_30.csv','w')\n",
    "#fil=open('Retail_31.csv','w')\n",
    "#fil=open('Retail_32.csv','w')\n",
    "#fil=open('Retail_33.csv','w')\n",
    "#fil=open('Retail_34.csv','w')\n",
    "#fil=open('Retail_35.csv','w')\n",
    "#fil=open('Retail_36.csv','w')\n",
    "#fil=open('Retail_37.csv','w')\n",
    "#fil=open('Retail_38.csv','w')  #####Random state=40,n_estimators=44,gb 0.3333333333333333 ,ls,mae,split=.11\n",
    "#fil=open('Retail_39.csv','w')  #####Random state=40,n_estimators=44,gb 0.3333333333333333 ,ls,mae,split=.10\n",
    "#fil=open('Retail_try.csv','w')     #####Random State=42,n_estimators=44,gb 0.3333333333333333 ,ls,mae,split=.10\n",
    "#fil=open('Retail_try1.csv','w')       #####Random State=42,n_estimators=45,gb 0.3333333333333333 ,ls,mae,split=.10\n",
    "#fil=open('Retail_40.csv','w')       #####Random State=42,n_estimators=44,gb 0.1111 ,ls,mae,split=.01,RMS=52\n",
    "#fil=open('Retail_41.csv','w')\n",
    "#fil=open('Retail_42.csv','w')\n",
    "fil=open('Retail_43.csv','w')     #####With macro data incorporated\n",
    "fil.write('Year,Sales(In ThousandDollars)\\n')\n",
    "for i in range(nsub.shape[0]-1):\n",
    "   fil.write('%d,%d\\n'%(i+1,nsub[i]))\n",
    "fil.write('%d,%d'%(i+2,nsub[i+1]))\n",
    "fil.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "big_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
